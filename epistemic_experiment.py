# -*- coding: utf-8 -*-
"""epistemic-slm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EZX68a49C7LOwdL_EQ9nHqgkp5u6Iml0
"""

!pip install scikit-learn peft transformers pandas pyyaml tqdm datasets logging

# --- Cell 1: Imports and Config ---
import os
import json
import torch
import pandas as pd
import numpy as np
from datasets import Dataset, load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Cell 2: Configuration ---

gpt2_target_modules = [
            "c_attn",  # Combined Q, K, V projection in GPT-2
            "c_proj",  # Output projection
            "c_fc",    # MLP first layer
        ]  # GPT-2/DialoGPT specific layer names

llama_target_modules = [
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        ]  # Target attention and MLP layers

config = {
    'model': {
        'name': 'microsoft/DialoGPT-small'
        # meta-llama/Llama-3.2-1B-Instruct
        # Qwen/Qwen2.5-0.5B-Instruct
        # apple/OpenELM-1_1B-Instruct
    },
    'lora_target_modules': gpt2_target_modules,
    'dataset': {
        'name': 'squad',
        'max_train_samples': 10000,
        'max_test_samples': 200
    },
    'training': {
        'epochs': 3,
        'batch_size': 8,
        'gradient_accumulation_steps': 2,
        'learning_rate': 5e-5,
        'warmup_steps': 100
    },
    'paths': {
        'output_dir': './epistemic_experiment_results_dialog_gpt'
    }
}
os.makedirs(config['paths']['output_dir'], exist_ok=True)

# --- Cell 3: Load Dataset ---
def load_qa_dataset(config):
    dataset_name = config['dataset']['name']
    logger.info(f"Loading dataset: {dataset_name}")
    if dataset_name == "squad":
        dataset = load_dataset("squad")
        train_data = dataset['train']
        test_data = dataset['validation']
        train_qa = [
            {'question': item['question'], 'answer': item['answers']['text'][0]}
            for item in train_data if item['answers']['text']
        ]
        test_qa = [
            {'question': item['question'], 'answer': item['answers']['text'][0]}
            for item in test_data if item['answers']['text']
        ]
    elif dataset_name == "natural_questions":
        dataset = load_dataset("natural_questions")
        train_qa = []
        test_qa = []
        for i, item in enumerate(dataset['train']):
            if i >= 10000: break
            if item['annotations']['short_answers']:
                train_qa.append({
                    'question': item['question']['text'],
                    'answer': item['annotations']['short_answers'][0]['text']
                })
        for i, item in enumerate(dataset['validation']):
            if i >= 2000: break
            if item['annotations']['short_answers']:
                test_qa.append({
                    'question': item['question']['text'],
                    'answer': item['annotations']['short_answers'][0]['text']
                })
    else:
        raise NotImplementedError(f"Dataset {dataset_name} not implemented")
    max_train = min(len(train_qa), config['dataset']['max_train_samples'])
    max_test = min(len(test_qa), config['dataset']['max_test_samples'])
    train_dataset = Dataset.from_list(train_qa[:max_train])
    test_dataset = Dataset.from_list(test_qa[:max_test])
    logger.info(f"Loaded {len(train_dataset)} training and {len(test_dataset)} test samples")
    return train_dataset, test_dataset

train_dataset, test_dataset = load_qa_dataset(config)

# --- Cell 4: Load Model and Tokenizer ---
model_name = config['model']['name']
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
def load_model(model_path=None):
    path = model_path or model_name
    logger.info(f"Loading model: {path}")
    model = AutoModelForCausalLM.from_pretrained(
        path,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True
    )
    return model

original_model = load_model()

# --- Cell 5: Generate Responses ---
def generate_responses(model, dataset, tokenizer):
    logger.info("Generating responses from model...")
    responses = []
    device = next(model.parameters()).device
    for item in tqdm(dataset, desc="Generating responses"):
        question = item['question']
        prompt = f"Question: {question}\nAnswer:"
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = full_response[len(prompt):].strip()
        responses.append(response)
    return responses

train_responses = generate_responses(original_model, train_dataset, tokenizer)

# --- Cell 6: Evaluate Responses ---
def evaluate_responses(responses, ground_truth):
    logger.info("Evaluating response correctness...")
    correct_flags = []
    for response, truth in zip(responses, ground_truth):
        response_lower = response.lower()
        truth_lower = truth.lower()
        truth_words = [word for word in truth_lower.split() if len(word) > 2]
        is_correct = any(word in response_lower for word in truth_words) if truth_words else False
        correct_flags.append(is_correct)
    accuracy = sum(correct_flags) / len(correct_flags)
    logger.info(f"Original model accuracy: {accuracy:.3f}")
    return correct_flags

ground_truth = [item['answer'] for item in train_dataset]
correct_flags = evaluate_responses(train_responses, ground_truth)

# --- Cell 7: Create Instruction Dataset ---
def create_instruction_dataset(train_dataset, responses, correct_flags, output_dir):
    logger.info("Creating instruction dataset...")
    instruction_data = []
    for item, response, is_correct in zip(train_dataset, responses, correct_flags):
        question = item['question']
        ground_truth = item['answer']
        target_answer = ground_truth if is_correct else "I don't know"
        instruction_data.append({
            'instruction': question,
            'input': '',
            'output': target_answer,
            'original_response': response,
            'was_correct': is_correct
        })
    # count the number of correctly found answers:
    num_correct = sum([instruction_data[i]['was_correct'] for i in range(len(instruction_data))])
    print("Known: ", num_correct)
    print("Unknown:", len(instruction_data)-num_correct)

    instruction_dataset = Dataset.from_list(instruction_data)
    instruction_dataset.save_to_disk(os.path.join(output_dir, 'instruction_dataset'))
    return instruction_dataset

instruction_dataset = create_instruction_dataset(train_dataset, train_responses, correct_flags, config['paths']['output_dir'])

# --- Cell 8: Prepare Training Data ---
def prepare_training_data(instruction_dataset):
    def format_example(example):
        prompt = f"Question: {example['instruction']}\nAnswer: {example['output']}"
        return {'text': prompt}
    formatted_dataset = instruction_dataset.map(format_example)
    return formatted_dataset

formatted_dataset = prepare_training_data(instruction_dataset)

# --- Cell 9: Tokenize Training Data ---
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        padding=True,
        max_length=512
    )
tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)

tokenized_dataset

# --- Cell 10: Fine-tune Model ---

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType

def get_precision_settings():
    if not torch.cuda.is_available():
        return {"fp16": False, "bf16": False}

    # Check if BF16 is supported (better for training)
    if torch.cuda.is_bf16_supported():
        return {"fp16": False, "bf16": True}

    # Fall back to FP32 to avoid FP16 gradient issues
    return {"fp16": False, "bf16": False}


def fine_tune_model(tokenized_dataset, tokenizer, config):
    logger.info("Starting fine-tuning with LoRA...")
    model = load_model()

    # Prepare model for training (gradient checkpointing, etc.)
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

    # Configure LoRA
    lora_config = LoraConfig(
        r=16,  # Rank of the low-rank matrices (higher = more parameters, try 8, 16, 32, 64)
        lora_alpha=32,  # Scaling factor (typically 2x the rank)
        target_modules=config.get('lora_target_modules'),
        lora_dropout=0.05,  # Dropout for LoRA layers
        bias="none",  # Don't train bias parameters
        task_type=TaskType.CAUSAL_LM,  # Causal language modeling
    )

    # Apply LoRA to the model
    model = get_peft_model(model, lora_config)

    # Print trainable parameters
    model.print_trainable_parameters()

    training_args = TrainingArguments(
        output_dir=os.path.join(config['paths']['output_dir'], 'finetuned_model'),
        overwrite_output_dir=True,
        num_train_epochs=config['training']['epochs'],
        per_device_train_batch_size=config['training']['batch_size'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
        warmup_steps=config['training']['warmup_steps'],
        learning_rate=config['training']['learning_rate'],
        **get_precision_settings(),
        logging_steps=50,
        save_strategy="epoch",
        save_total_limit=2,
        prediction_loss_only=True,
        report_to="none", # Disable wandb reporting
        remove_unused_columns=True,
    )

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    trainer.train()

    # Save the LoRA adapters
    finetuned_path = os.path.join(config['paths']['output_dir'], 'finetuned_model')
    model.save_pretrained(finetuned_path)
    tokenizer.save_pretrained(finetuned_path)

    logger.info(f"LoRA adapters saved to {finetuned_path}")

    return finetuned_path
finetuned_model_path = fine_tune_model(tokenized_dataset, tokenizer, config)

# --- Cell 11: Compare Models ---
def compare_models(test_dataset, tokenizer, finetuned_model_path):
    logger.info("Comparing original and fine-tuned models...")
    original_model = load_model()
    finetuned_model = load_model(finetuned_model_path)
    original_responses = generate_responses(original_model, test_dataset, tokenizer)
    finetuned_responses = generate_responses(finetuned_model, test_dataset, tokenizer)
    ground_truth = [item['answer'] for item in test_dataset]
    original_correct = evaluate_responses(original_responses, ground_truth)
    finetuned_correct = evaluate_responses(finetuned_responses, ground_truth)
    original_idk_count = sum(1 for r in original_responses if "don't know" in r.lower())
    finetuned_idk_count = sum(1 for r in finetuned_responses if "don't know" in r.lower())
    finetuned_idk_on_wrong = 0
    finetuned_idk_on_correct = 0
    for orig_correct, ft_response in zip(original_correct, finetuned_responses):
        if "don't know" in ft_response.lower():
            if not orig_correct:
                finetuned_idk_on_wrong += 1
            else:
                finetuned_idk_on_correct += 1
    results = {
        'original_accuracy': sum(original_correct) / len(original_correct),
        'finetuned_accuracy': sum(finetuned_correct) / len(finetuned_correct),
        'original_idk_count': original_idk_count,
        'finetuned_idk_count': finetuned_idk_count,
        'finetuned_idk_on_wrong': finetuned_idk_on_wrong,
        'finetuned_idk_on_correct': finetuned_idk_on_correct,
        'total_originally_wrong': sum(1 for c in original_correct if not c),
        'responses': {
            'original': original_responses,
            'finetuned': finetuned_responses,
            'ground_truth': ground_truth,
            'original_correct': original_correct,
            'finetuned_correct': finetuned_correct
        }
    }
    return results

results = compare_models(test_dataset, tokenizer, finetuned_model_path)

# --- Cell 12: Analyze Results ---
def analyze_results(results):
    logger.info("Analyzing results...")
    total_wrong_original = results['total_originally_wrong']
    idk_on_wrong = results['finetuned_idk_on_wrong']
    idk_on_correct = results['finetuned_idk_on_correct']
    humility_precision = idk_on_wrong / (idk_on_wrong + idk_on_correct) if (idk_on_wrong + idk_on_correct) > 0 else 0
    humility_recall = idk_on_wrong / total_wrong_original if total_wrong_original > 0 else 0
    humility_f1 = 2 * (humility_precision * humility_recall) / (humility_precision + humility_recall) if (humility_precision + humility_recall) > 0 else 0
    analysis = {
        'epistemic_humility_precision': humility_precision,
        'epistemic_humility_recall': humility_recall,
        'epistemic_humility_f1': humility_f1,
        'accuracy_change': results['finetuned_accuracy'] - results['original_accuracy'],
        'idk_increase': results['finetuned_idk_count'] - results['original_idk_count'],
        'conclusions': []
    }
    if humility_recall > 0.5:
        analysis['conclusions'].append("Model successfully learned to express uncertainty for many questions it originally answered incorrectly.")
    else:
        analysis['conclusions'].append("Model showed limited improvement in expressing uncertainty for incorrect answers.")
    if humility_precision > 0.7:
        analysis['conclusions'].append("Model demonstrates good precision in epistemic humility - mostly says 'I don't know' appropriately.")
    else:
        analysis['conclusions'].append("Model may be over-generalizing 'I don't know' responses to questions it could answer correctly.")
    if analysis['accuracy_change'] > 0:
        analysis['conclusions'].append("Fine-tuning improved overall accuracy.")
    elif analysis['accuracy_change'] < -0.05:
        analysis['conclusions'].append("Fine-tuning significantly reduced overall accuracy - may need adjustment.")
    return analysis

analysis = analyze_results(results)

# --- Cell 13: Visualizations ---
def create_visualizations(results, analysis, output_dir):
    logger.info("Creating visualizations...")
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    accuracies = [results['original_accuracy'], results['finetuned_accuracy']]
    axes[0, 0].bar(['Original', 'Fine-tuned'], accuracies, color=['blue', 'orange'])
    axes[0, 0].set_title('Model Accuracy Comparison')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_ylim(0, 1)
    idk_counts = [results['original_idk_count'], results['finetuned_idk_count']]
    axes[0, 1].bar(['Original', 'Fine-tuned'], idk_counts, color=['blue', 'orange'])
    axes[0, 1].set_title('"I Don\'t Know" Response Counts')
    axes[0, 1].set_ylabel('Count')
    categories = ['IDK on Wrong', 'IDK on Correct']
    values = [results['finetuned_idk_on_wrong'], results['finetuned_idk_on_correct']]
    axes[1, 0].bar(categories, values, color=['green', 'red'])
    axes[1, 0].set_title('Fine-tuned Model: "I Don\'t Know" Usage')
    axes[1, 0].set_ylabel('Count')
    metrics = ['Precision', 'Recall', 'F1']
    metric_values = [
        analysis['epistemic_humility_precision'],
        analysis['epistemic_humility_recall'],
        analysis['epistemic_humility_f1']
    ]
    axes[1, 1].bar(metrics, metric_values, color=['purple', 'brown', 'pink'])
    axes[1, 1].set_title('Epistemic Humility Metrics')
    axes[1, 1].set_ylabel('Score')
    axes[1, 1].set_ylim(0, 1)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'results_visualization.png'), dpi=300, bbox_inches='tight')
    plt.close()

create_visualizations(results, analysis, config['paths']['output_dir'])

# --- Cell 14: Save Results and Report ---
def save_results(results, analysis, config):
    logger.info("Saving results...")
    summary_results = {k: v for k, v in results.items() if k != 'responses'}
    summary = {
        'results': summary_results,
        'analysis': analysis,
        'config': config
    }
    with open(os.path.join(config['paths']['output_dir'], 'experiment_summary.json'), 'w') as f:
        json.dump(summary, f, indent=2)
    with open(os.path.join(config['paths']['output_dir'], 'detailed_results.json'), 'w') as f:
        json.dump(results, f, indent=2)
    # Human-readable report
    report = f"""
# Epistemic Humility Experiment Report

## Experiment Configuration
- Model: {config['model']['name']}
- Dataset: {config['dataset']['name']}
- Training samples: {config['dataset']['max_train_samples']}
- Test samples: {config['dataset']['max_test_samples']}

## Results Summary

### Accuracy Metrics
- Original model accuracy: {results['original_accuracy']:.3f}
- Fine-tuned model accuracy: {results['finetuned_accuracy']:.3f}
- Accuracy change: {analysis['accuracy_change']:.3f}

### Epistemic Humility Metrics
- Precision: {analysis['epistemic_humility_precision']:.3f}
- Recall: {analysis['epistemic_humility_recall']:.3f}
- F1 Score: {analysis['epistemic_humility_f1']:.3f}

### "I Don't Know" Response Analysis
- Original model "IDK" responses: {results['original_idk_count']}
- Fine-tuned model "IDK" responses: {results['finetuned_idk_count']}
- "IDK" on originally wrong answers: {results['finetuned_idk_on_wrong']}
- "IDK" on originally correct answers: {results['finetuned_idk_on_correct']}

### Key Insights
"""
    for conclusion in analysis['conclusions']:
        report += f"- {conclusion}\n"
    with open(os.path.join(config['paths']['output_dir'], 'experiment_report.md'), 'w') as f:
        f.write(report)

save_results(results, analysis, config)